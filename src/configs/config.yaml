defaults:
  - model: transformer
  - writer : visualizer
  - dataset: translation
  - _self_


experiment:
  name: "translation_experiment"
  seed: 42
  description: "Training transformer for EN-RU translation"

logging:
  level: "INFO"
  wandb_project: "translation_experiments"
  log_git_info: True

optimizer:
  _target_: torch.optim.Adam
  lr: 3e-4
lr_scheduler:
  _target_: torch.optim.lr_scheduler.StepLR
  gamma: 0.9
  step_size: ${trainer.epoch_len}
loss_function:
  _target_: src.loss.CrossEntropyLoss
trainer:
  log_step: 50
  resume_from: null
  device: auto
  save_dir: "saved"
  num_epochs: 5
  seed: 42
  override: True
  epoch_len: 1000 